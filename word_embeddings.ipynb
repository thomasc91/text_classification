{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# Testing various models with word embeddings"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport time\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "752c403381f8bcd1772dd5c9e6edd5f09d9f113a"
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "339cab80b71d4dc62f407642a969f735ba06fb2c"
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3468cc0e36a9aea9997bab8a5cc25c8b3e0e600"
      },
      "cell_type": "code",
      "source": "from gensim.models import KeyedVectors\nnews_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa6ec157e82f76c25b3fbc08a94402eb4ccad7dc"
      },
      "cell_type": "code",
      "source": "embeddings_index.most_similar(positive=['King', 'woman'], negative = ['man'], topn = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cd6062dda6e3c36f77d4bcb7c730294125094e7"
      },
      "cell_type": "code",
      "source": "embeddings_index.most_similar(positive=['Australia', 'pizza'], negative = ['Italy'], topn = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "561feea114e9fdc08c69bca3eb4bb4329254d8ee"
      },
      "cell_type": "code",
      "source": "embeddings_index.most_similar(positive=['biked', 'today'], negative = ['yesterday'], topn = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "cb13cbe626ddb1705d396a5b9674c7c03739ddb9"
      },
      "cell_type": "code",
      "source": "embeddings_index.most_similar(positive=['Africa', 'pizza'], negative = ['Italy'], topn = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "8a07bf1fe9599312bd03db151ab9bd3e0c3c5ac2"
      },
      "cell_type": "code",
      "source": "embeddings_index.most_similar(positive=['Brisbane', 'UK'], negative = ['Australia'], topn = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "1a0cd82eba10935bd4fa074e3a71d2bd4f8f9432"
      },
      "cell_type": "code",
      "source": "vector = embeddings_index['Maccas']\nvector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de688a644dd389699611740c773515cab8ef1d92"
      },
      "cell_type": "markdown",
      "source": "**Data pre-processing for using word vectors**\n* Split training data into train and validation samples. \n* Fill missing values.\n* Tokenise the individual words in text so we can convert the words into vectors\n* Pad each sentence so the model can be fed vectors of the same size. For longer sentences, the number of words will be truncated to 'max_len' and for shorter sentences the sequence will be padded with zeroes. "
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "258c618555718721699d7d814b467ca6810ea99d"
      },
      "cell_type": "code",
      "source": "#Split data to training and validation sets\ntrain_df, val_df = train_test_split(train_df, test_size = 0.1, random_state=2019)\n\n#Configure the word embedding values\nembed_size = 300 #size of each word vector (number of columns in the embedding matrix)\nmax_features = 50000 #number of unique words (number of rows in the embedding matrix)\nmaxlen = 100 #maximum number of words in each sentence\n\n#Fill missing values\ntrain_X = train_df['question_text'].fillna('_na_').values\nval_X = val_df['question_text'].fillna('_na_').values\ntest_X = test_df['question_text'].fillna('_na_').values\n\n#Tokenise sentences\n# Tokenizer is the class for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\ntokeniser = Tokenizer(num_words = max_features) \ntokeniser.fit_on_texts(list(train_X)) #fit on our training set\n\n#Change the input vectors to lists of numbers corresponding to the word indexes\ntrain_X = tokeniser.texts_to_sequences(train_X)\nval_X = tokeniser.texts_to_sequences(val_X)\ntest_X = tokeniser.texts_to_sequences(test_X)\n\n#Pad the sentences\ntrain_X = pad_sequences(train_X, maxlen = maxlen)\nval_X = pad_sequences(val_X, maxlen = maxlen)\ntest_X = pad_sequences(test_X, maxlen = maxlen)\n\n#Get target values (sincere or insincere in our case)\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "50be7f2508ff2202e5c6f18318790897d066091f"
      },
      "cell_type": "markdown",
      "source": "**Training our own embeddings**\n\nThe contents of the table that relates indexes to embedding vectors (i.e., the weights of the embedding layer) are initialized at random and then optimized by the training algorithm (e.g., Gradient Descent). \n****\nThis type of learned embedding is different to an embedding learned through word2vec. Word2vec aims to capture the semantic similarity between words. The embedding trained from scratch is only useful for the particular classification problem.\n\nFor example, the following image taken from [this paper](https://link.springer.com/article/10.1007/s10489-017-1109-7) shows the embedding of three sentences with a Keras Embedding layer trained from scratch as part of a supervised network designed to detect clickbait headlines (left) and pre-trained word2vec embeddings (right). The word2vec embeddings reflect the semantic similarity between phrases b) and c). Conversely, the embeddings generated by Keras's Embedding layer might be useful for classification, but do not capture the semantical similarity of b) and c).\n\n![](https://i.stack.imgur.com/BNIsVl.png)\n\n ## Basic Keras Model training our own embeddings"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0581c80592422bf0e9eabeee97ad57496d2be94",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras import layers\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=max_features,\n                           output_dim=embed_size,\n                           input_length=maxlen))\nmodel.add(layers.GlobalMaxPool1D()) #takes the maximum of all features \nmodel.add(layers.Dense(10, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7955f1d24cf70294d75b5b19e7004f6956f1354d"
      },
      "cell_type": "code",
      "source": "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data = (val_X, val_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f958c34377c8f1467858eba3616abc3dafdb43d8",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))\n    \n#Best F1 score at threshold 0.29 is 0.6441116865570817 ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c2723e9399180a7e90011c43f216f81eb92842d"
      },
      "cell_type": "markdown",
      "source": "## Basic Keras Model with pre-trained FastText Embeddings"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "831304ac4439e1ca1fcdcef5f5388fbb123c8703"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokeniser.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7718c0afbe0ea522b82330ace73e6adc8fbe3416"
      },
      "cell_type": "code",
      "source": "np.shape(embedding_matrix_2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42e8fa685f56fa036f57ea3d2439e1361e530dc8"
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras import layers\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=max_features,\n                           output_dim=embed_size,\n                           input_length=maxlen\n                          ,weights=[embedding_matrix_2]\n                          ,trainable=False))\nmodel.add(layers.GlobalMaxPool1D()) #takes the maximum of all features \nmodel.add(layers.Dense(10, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "c72bffd11c01842a3c50fd440cf9ee6ecd4b6b1a"
      },
      "cell_type": "code",
      "source": "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data = (val_X, val_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "255ac8567056d5b31aaf81c8d041310142af6cf7"
      },
      "cell_type": "code",
      "source": "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))\n    \n#Best F1 score at threshold 0.26 is 0.526683967054045",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "017777cb3fcd65b0ad262b7096456e2fe506cde3"
      },
      "cell_type": "markdown",
      "source": "## Basic Keras model with re-trainable pretrained embeddings"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90131c2c0e255ca65e78508ec4b6d96ae0970288"
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras import layers\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=max_features,\n                           output_dim=embed_size,\n                           input_length=maxlen\n                          ,weights=[embedding_matrix_2]\n                          ,trainable=True))\nmodel.add(layers.GlobalMaxPool1D()) #takes the maximum of all features \nmodel.add(layers.Dense(10, activation='relu')) \nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "a42405039f9f700d549d538bdbece5ea87c1fbd7"
      },
      "cell_type": "code",
      "source": "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data = (val_X, val_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "e931b35fcc7c6c7d06fa2b2f09d690e6e82af526"
      },
      "cell_type": "code",
      "source": "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))\n    \n#Best F1 score at threshold 0.35 is 0.6577158774373258",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "40f30108b6a38ea20ec6ad2d0c768288e2dd00db"
      },
      "cell_type": "markdown",
      "source": "## Bidirectional GRU model (non-trainable, pre-trained FastText embeddings)"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "333bf964ed63101e8895d1052f7d746aad7713e7"
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix_2], trainable=False)(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "50e5a69f62bf44874d69ae5514b62a85630666a9"
      },
      "cell_type": "code",
      "source": "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data = (val_X, val_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "8ba73b6e45a17d5367ec8cd469d89e3141914a0c"
      },
      "cell_type": "code",
      "source": "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))\n    \n# Best F1 score at threshold 0.31 is 0.664618760285244",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "154d91da94c7f5af4b19db7dc9df9d230122ad77"
      },
      "cell_type": "markdown",
      "source": "## Bidirectional GRU model (trainable, pre-trained FastText embeddings)"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "9d4276e4f1e8da0511892b35525547d12b88ae67"
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix_2] ,trainable=True)(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d331615da581bce77a5d8a9f5dfdff65a8fd347e"
      },
      "cell_type": "code",
      "source": "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data = (val_X, val_y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4460d2e0bd2b3a3531f75debc3a604ad9b67d0a9"
      },
      "cell_type": "code",
      "source": "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))\n    \n# Best F1 score at threshold 0.3 is 0.6725162436830121",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b08355bb5e1936e9e8c615d05aaaa30d1777af75"
      },
      "cell_type": "code",
      "source": "pred_test_y = model.predict([test_X], batch_size=1024, verbose=1)\npred_test_y = (pred_test_y>0.44).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
