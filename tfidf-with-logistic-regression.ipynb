{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\n\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n\nprint('Train shape : ', train_df.shape)\nprint('Test shape : ', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f72726ce237c3d713cc997de0c31999c7578dd59"},"cell_type":"markdown","source":"## Text vectorisation\nTo apply machine learning/predictive models to text data, we need to transform the unstructured data into a structured form.\n\nFrom 'Applied Text Analysis with Python' by Tony Ojeda, Rebecca Bilbro, Benjamin Bengfort:\n\n*Machine learning algorithms operate on a numeric feature space, expecting input as a two-dimensional array where rows are instances and columns are features. In order to perform machine learning on text, we need to transform our documents into vector representations such that we can apply numeric machine learning. This process is called feature extraction or more simply, vectorization, and is an essential first step toward language-aware analysis.*\n\nThere are a number of vectorisation methods that we will go through below.\n\n#### Bag of words\nThe basic approach to text vectorisation is the 'bag of words' method. First, a fixed length vector is defined where each entry corresponds to a pre-defined dictionary of words. The size of the vector is the same size as the dictionary. \n\nThe entry for each word in the vector is the number of times that word appears in the text. For example, if our dictionary contains the words {MonkeyLearn, is, the, not, great}, and we want to vectorize the text 'MonkeyLearn is great', we would have the following vector: (1, 1, 0, 0, 1). \n\n#### Term frequency-inverse document frequency (TFIDF)\nA limitation of the bag of words approach is that the technique doesn't capture the meaning of the text, or the context in which the words appear. A single word occuring in a document may be very important, but the noise of frequently occuring words does not allow the word to have an appropriate weighting. TFIDF is a technique for weighting the relative importance of a single word or n-gram.\n\nFrom Wikipedia:\n\nIn information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.\n\nThe output of TFIDF is a vector of numbers per document, where the number for each word the word's TFIDF weighting."},{"metadata":{"trusted":true,"_uuid":"898ad982d8e2b5e4e49f26d086e426fb582287ab"},"cell_type":"code","source":"from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n\n#Learn vocabulary and idf, return term-document matrix\ntfidf_vec.fit_transform(train_df['question_text'].values.tolist() + test_df['question_text'].values.tolist())\n\ntrain_tfidf = tfidf_vec.transform(train_df['question_text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['question_text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"754ee98876d8c48626d907d40abb2099a55b3b96"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nimport numpy as np\nkf = KFold(n_splits = 5, shuffle=True, random_state=2017)\n\ntrain_y = train_df['target'].values\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n    model = LogisticRegression(C=5, solver = 'sag') #sag solver better for large datasets\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)[:,1]\n    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n    return pred_test_y, pred_test_y2, model\n\n# We split the training set into a dev index (to train the model) and a val index\n# (to validate/test the model)\n\nprint(\"Building model.\")\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0]])\n#The index splits are then applied to the training TFIDF matrix \nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index] = pred_val_y \n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2badb2321be1e534a0944ae5d4ef808601ef886"},"cell_type":"code","source":"for thresh in np.arange(0.12, 0.20, 0.01):\n    thresh = np.round(thresh, 2)\n    print('F1 score at threshold {0} is {1}'.format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))\n    print('Precision score at threshold {0} is {1}'.format(thresh, metrics.precision_score(val_y, (pred_val_y>thresh).astype(int))))\n    print('Recall score at threshold {0} is {1}'.format(thresh, metrics.recall_score(val_y, (pred_val_y>thresh).astype(int))))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"772fbd9fd1d9f0b13a8443c4b1b25df1c2540942"},"cell_type":"code","source":"out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_full_test\nthreshold = 0.17\nout_df['prediction'] = np.where(out_df['prediction'] > threshold, 1,0)\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}